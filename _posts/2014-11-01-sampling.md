---
layout: default
title: Sampling - A Summary
categories: machine-learning
---

To summarize the sampling methods mentioned in Pattern Recognition and Machine Learning.



### Rejection sampling
 
As is often the case, the exact probability of $$ z $$ is not easy to estimate, but the unnormalized probability $$\hat{p}(z)$$ can be calucated directly. To estimate this unnormalized probability distribution, we need a *proposal distribution* $$q(z)$$ and a constant $$k$$ that for all value of $$z$$, $$kq(z) \le \hat{p}(z)$$.

Everytime a number $$z_0$$ is generated, we accept it with the probability of $$\hat{q}(z_0)/kq(z_0)$$. It's obvious that the resulting distribution is equavalient to $$p(z)$$. The overall accept probability of this method is $$p(accept)=\int (\hat{p}(x)/kq(x))*q(x)dx=\frac{1}{k}\int \hat{p}(x)dx$$. Sometimes, the *proposal distribution* can not be approximated well, we can adopt the adaptive method. By this method, we construct an envolope over $$\hat{p}(x)$$.

### Importance sampling

The idea behind the rejection is that we can estimate a distribution that is similar with target distribution. More accurate it is, less samplings will be abandoned. Consequencely, the disadvantage is also quite severe. 

Another sampling method which is similar with rejection sampling method is importance sampling. In contrast to rejection sampling, It will never reject any sampling number, but weight the sampling with respect of its 'importance' factor. Let's see the equation $$ E(f)=\int f(z)p(z)dz=\int f(z)\frac{p(z)}{q(z)}q(z)dz $$. If we sampling with distribution $$q(z)$$, $$E(f)$$ can be approximated by $$E(f)=\frac{1}{L}\sum f(z)\frac{p(z)}{q(z)} $$. As we can see from this equation, with the help of importance factor $$\frac{p(z)}{q(z)}$$, the numbers sampled with q(z) will remain. Actually, the idea of importance sampling is quite similar with rejection. In rejection sampling, we reject numbers often when the q(z) differs a lot from $$\hat{p}(z)$$, resulting in a low efficiency; in importance sampling, the importance of samples decreases when q(z) doesn't fit p(z) well and affect the sampling efficiency. 
If p(z) is unaccesible, we can obtain a little bit different equation: $$ E(f)=\int f(z)p(z)dz=\frac{Z_q}{Z_p}\int f(z)\frac{\hat{p}(z)}{\hat{q}(z)}q(z)dz $$. Calculating $$\frac{Z_q}{Z_p}$$ is a little tricky, we can get it via the same sampling set -- $$\frac{Z_p}{Z_q}=\int \hat{p}(z)/\hat{q}(z)q(z)dz$$. Hence, $$E(f)$$ can be presented as $$E(f)=\sum{w_if(z_i)}, w_i=\frac{\hat{p}(z_i)\hat{q}(z_i)}{\sum{\hat{p}(z_i)\hat{q}(z_i)}}$$.

### Sampling-importance-resampling

The success of rejection sampling is determined by constant $$k$$. Combining two methods mentioned above, sampling-importance-resampling method is proposed to tackle this problem. At first, we sample points with a *proposal distribution* $$q(z)$$ and we can calculate the weight of each sample. Then we select sample by their weight. The resulting sampling distribution is proved to converge to target distribution $$p(z)$$. $$p(z \le a)=\sum_{l:z_l \le a}{w_l}=\frac{\sum_l{I(z_l \le a)\hat{p}(z_l)/q(z_l)}}{\sum_l{\hat{p}(z_l)/q(z_l)}}$$, with number increasing, the probability will be $$\frac{\sum_l{I(z_l \le a)\hat{p}(z)/q(z)q(z)dz}}{\int{\hat{p}(z)/q(z)q(z)dz}}=\int{I(z\le a)p(z)dz}$$

### Sampling and EM algorithm

EM algorithm review: the goal is to minimize the likelihood function $$L(\theta)=\sum_i{log{\sum_zP(z\|x_i,\theta)P(x_i,z\|\theta)}}$$. At first E step, we need to estimate the posterior probability of latent parameter $$p(z\|x, \theta)$$. Secondly, we need to maximize $$\sum_i\sum_{z_i}p(z_i\|x_i,\theta)logP(x_i\|z_i,\theta)$$. For simplicity, the inference of EM algorithm is not included here. In particular, sampling methods can be used to approximate the E step of the EM algorithm for models in which the E step cannot be performed analytically.We can use sampling methods to approximate this integral by a finite sum over samples {$$z(l)$$}, which are drawn from the current estimate for the posterior distribution $$p(z\|x, \theta^{old})$$, so that $$Q(\theta, \theta^{old})=1/L\sum_l{lnp(z(l), x\|\theta)}$$.

We can apply this method to a full bayesian treatment in which we wish to sample from the posterior distribution over the parameter vector $$\theta$$.

* **IP algorithm**
* I-step. We wish to sample from $$p(z\|x)$$ but we cannot do this directly. We therefore note the relation $$p(z\|x)=\int p(z\|θ,x)p(θ\|x)dθ$$ and hence for l = 1,...,L we first drawa sample $$θ(l)$$ from the current estimate for $$p(θ\|x)$$, and then use this to draw a sample $$z(l)$$ from $$p(z\|θ(l), x)$$. 
* P-step. Given the relation $$p(θ\|x)=\int p(θ\|z,x)p(z\|x)dz$$ we use the samples $${z(l)}$$ obtained from the I-step to compute a revised estimate of the posterior distribution over θ given by $$p(\theta \|x)=1/L\sum_l{p(\theta\|z_l,x)}$$.

### Markov Chain Monte Carlo

Markov Chain Monte Carlo can deal with a large amount of high dimension distributions. Before getting into this algorithm, we need to get familiar with Markov Chain. Markov Chain is a set of variables only depending on its previous one. If a Markov Chain is homogeneous and reversible, it will converge to a single invariant distribution. 

* Homogeneous: A Markov chain is called homogeneous if the transition probabilities are the same for all $$m$$.
* Reversible: A Markov chain that respects detailed balance is said to be reversible. $$p^*(z)T(z,z') = p^*(z')T(z',z)$$.

This two properties are the foundament of MCMC method. Then let's turn back to sampling algorithm. If we are giving a unnormalized distribution $$\hat{p}$$, similar with other sampling method, we need a *proposal distribution* $$q$$. But in contrast, this proposal distribution generates variables on the condition of other variable, i.e. $$q(z\|z')$$. At time $$i$$, we generate a new variable $$z^*$$ with $$q(z\|z_{i-1})$$ and accept with probability $$min(1, \frac{\hat{p}(z^*)q(z_{i-1}\|z^*))}{\hat{p}(z_i)q(z^*\|q_{i-1})})$$. If it's accepted, $$z_{i}$$ is set to $$z^*$$, otherwise, $$z_{i}$$ is set to $$z_{i-1}$$. It's easy to prove that the generated sampling tends to $$p$$ distribution by proving it satisfies two properties mentioned above. Obviously, it is homogeneous because it is invariant to time. And 

* $$p(z)q(z\|z')A(z',z) = min(p(z)q(z\|z'),p(z')q(z'\|z))$$ 
$$=min(p(z')q(z'\|z),p(z)q(z\|z')) = p(z')qk(z'\|z)A(z, z')$$
	
, which is *reversible* property. This algorithm is called *Metropolis-Hastings* method. If $$q$$ is symmetric, the accept probability becomes $$min(1, \frac{\hat{p}(z^*)}{\hat{p}(z_i)})$$ and is called *Metropolis* algoritm.

### Gibbs sampling

Gibbs sampling can be viewed as a special case of Metropolis-Hastings algorithm. When using its own transfer probability $$\hat{p}(x_i\|x_{!i})$$ as proposal distribution $$\hat{q}(x)$$, the accept probability will be 1 ($$A(z^*, z) = \frac{p(z^*)q_k(z\|z^*)}{p(z)q_k(z^*\|z)} = \frac{p(z^*_{!k}\|z^*_k)p(z^*_k)p(z\|z^*)}{p(z_{!k}\|z_k)p(z_k)p(z^*_k\|z_{!k})}$$ and $$p(z_{!k}) == p(z^*_{!k})$$). To see the procedure of Gibbs Sampling, refer to PRML, page-543.

### Slice sampling

There are two ways available to tackle the problem that the sampling process is sensitive to the step size caused by Metropolis algorithm. The first one is slice sampling. In this method, we iteratively change the sampling point and slice height looking forward to sampling far away from the previous one. It is easy to  prove that slice sampling is right ~

### Hybrid Monte Carlo

Another way is hybrid monte carlo algorithm, the idea is to use the stable state in physical dynamtic system to obtain a feasible distribution. When we take the kinetic energy (the rate of change of the state) into consideration, the global system energy $$H(z, r)$$ will remain stable as well as system volume while $$E(z)$$ changes over time. The Metropolis algorithm can deal with the calculation error and make detailed balance by setting accept probability as $$min(1, exp{H(z,r) - H(z^*, r^*)})$$.
